{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Operationalization & Deployment\n",
    "\n",
    "In the `Code\\2_model_building_and_evaluation.ipynb` notebook, we trained an LSTM neural net to predict aircraft engine failure 30 cycles into the future.\n",
    "\n",
    "In this notebook, we will create the artifacts and scripts to deploy the LSTM model into a webservice on Azure. The artifacts include the model files, and test scripts to validate your model can be used to predict future reliability of the engines based on the present operating characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# import the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import h5py\n",
    "\n",
    "# For creating the deployment schema file\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "run_logger = get_azureml_logger()\n",
    "run_logger.log('amlrealworld.predictivemaintenanceforpm.operationalization','true')\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model storage \n",
    "\n",
    "We will store the model and operationalization artifacts in an Azure Blob Storage Container for easy retrieval to your deployment platform.\n",
    "\n",
    "Instructions for setting up your Azure Storage account are available within this link (https://docs.microsoft.com/en-us/azure/storage/blobs/storage-python-how-to-use-blob-storage). You will need to copy your account name and account key from the _Access Keys_ area in the portal into the following code block. These credentials will be reused in all four Jupyter notebooks.\n",
    "\n",
    "We will handle creating the containers and writing the data to these containers for each notebook. Further instructions for using Azure Blob storage with AML Workbench are available (https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md).\n",
    "\n",
    "You will need to enter the **ACCOUNT_NAME** as well as the **ACCOUNT_KEY** in order to access Azure Blob storage account you have created. This notebook will create and store all the resulting data files in a blob container under this account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the Data Ingestion and Preparation notebook is stored in the sensordata ingestion container.\n",
    "MODEL_CONTAINER = \"pmlstmmodel\" \n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables tell the notebook where to store and load files for building and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store each of these data sets in a local persistance folder\n",
    "SHARE_ROOT = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']\n",
    "\n",
    "# These file names detail the data files. \n",
    "TEST_DATA = 'PM_test_files.pkl'\n",
    "\n",
    "# We'll serialize the model in json format\n",
    "LSTM_MODEL = 'modellstm.json'\n",
    "\n",
    "# and store the weights in h5\n",
    "MODEL_WEIGHTS = 'modellstm.h5'\n",
    "\n",
    "# and the schema file\n",
    "SCHEMA_FILE = 'service_schema.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data frame\n",
    "\n",
    "We have previously stored the test data frame in the local persistence directory indicated by the `SHARE_ROOT` variable. We'll use this test data frame to build the model schema to describe the deployment function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>cycle_norm</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.310661</td>\n",
       "      <td>0.269413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.661834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150602</td>\n",
       "      <td>0.379551</td>\n",
       "      <td>0.222316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682171</td>\n",
       "      <td>0.686827</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376506</td>\n",
       "      <td>0.346632</td>\n",
       "      <td>0.322248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.721348</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370482</td>\n",
       "      <td>0.285154</td>\n",
       "      <td>0.408001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.662110</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391566</td>\n",
       "      <td>0.352082</td>\n",
       "      <td>0.332039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658915</td>\n",
       "      <td>0.716377</td>\n",
       "      <td>0.011080</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271084</td>\n",
       "      <td>0.176150</td>\n",
       "      <td>0.217421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.596899</td>\n",
       "      <td>0.624827</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271084</td>\n",
       "      <td>0.268149</td>\n",
       "      <td>0.381330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550388</td>\n",
       "      <td>0.691798</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400602</td>\n",
       "      <td>0.214737</td>\n",
       "      <td>0.314652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.705426</td>\n",
       "      <td>0.591273</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201807</td>\n",
       "      <td>0.485066</td>\n",
       "      <td>0.506921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.770367</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.309789</td>\n",
       "      <td>0.276671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.565891</td>\n",
       "      <td>0.673571</td>\n",
       "      <td>0.024931</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n",
       "0   1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661  0.269413   \n",
       "1   1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551  0.222316   \n",
       "2   1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632  0.322248   \n",
       "3   1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154  0.408001   \n",
       "4   1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082  0.332039   \n",
       "5   1      6  0.568966  0.750000       0.0  0.0  0.271084  0.176150  0.217421   \n",
       "6   1      7  0.500000  0.666667       0.0  0.0  0.271084  0.268149  0.381330   \n",
       "7   1      8  0.534483  0.500000       0.0  0.0  0.400602  0.214737  0.314652   \n",
       "8   1      9  0.293103  0.500000       0.0  0.0  0.201807  0.485066  0.506921   \n",
       "9   1     10  0.356322  0.416667       0.0  0.0  0.259036  0.309789  0.276671   \n",
       "\n",
       "    s5   ...    s16       s17  s18  s19       s20       s21  cycle_norm  RUL  \\\n",
       "0  0.0   ...    0.0  0.333333  0.0  0.0  0.558140  0.661834    0.000000  142   \n",
       "1  0.0   ...    0.0  0.416667  0.0  0.0  0.682171  0.686827    0.002770  141   \n",
       "2  0.0   ...    0.0  0.416667  0.0  0.0  0.728682  0.721348    0.005540  140   \n",
       "3  0.0   ...    0.0  0.250000  0.0  0.0  0.666667  0.662110    0.008310  139   \n",
       "4  0.0   ...    0.0  0.166667  0.0  0.0  0.658915  0.716377    0.011080  138   \n",
       "5  0.0   ...    0.0  0.333333  0.0  0.0  0.596899  0.624827    0.013850  137   \n",
       "6  0.0   ...    0.0  0.250000  0.0  0.0  0.550388  0.691798    0.016620  136   \n",
       "7  0.0   ...    0.0  0.416667  0.0  0.0  0.705426  0.591273    0.019391  135   \n",
       "8  0.0   ...    0.0  0.250000  0.0  0.0  0.744186  0.770367    0.022161  134   \n",
       "9  0.0   ...    0.0  0.250000  0.0  0.0  0.565891  0.673571    0.024931  133   \n",
       "\n",
       "   label1  label2  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       0       0  \n",
       "4       0       0  \n",
       "5       0       0  \n",
       "6       0       0  \n",
       "7       0       0  \n",
       "8       0       0  \n",
       "9       0       0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_pickle(SHARE_ROOT + TEST_DATA)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to recreate the feature engineering (creating the sequence features) just as we did in the model building step. We will do this within the webservice so that the service can take the raw sensor data, and return a scored result predicting probability of failure at 30 days (`label1`). \n",
    "\n",
    "When scoring an unseen observation, the model will not know the true labels. Therefore, we create a `score_df` without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the feature columns \n",
    "# Sequence help order the observations in \"time\"\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "# key columns group the machines\n",
    "key_cols = ['id', 'cycle']\n",
    "\n",
    "# Labels are what we're predicting.\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "# The scoreing data should not have labels... if we knew the label, \n",
    "# we wouldn'y need to predict.\n",
    "score_df = test_df.drop(label_cols, axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test init() and run() functions\n",
    "\n",
    "The web service requires two functions, an `init()` function that will initialize the web service by loading the model into the service, and a `run()` function that will engineer the features to match the model call structure, and score that data set. We create the functions in here for testing and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "    global loaded_model\n",
    "    \n",
    "    # load json and create model\n",
    "    with open(SHARE_ROOT + LSTM_MODEL, 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(os.path.join(SHARE_ROOT, MODEL_WEIGHTS))\n",
    "    loaded_model.compile('sgd','mse')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(score_input): \n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "\n",
    "    # Feature engineering\n",
    "    input_features = score_input.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_input[score_input['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                 for id in score_input['id'].unique() if len(score_input[score_input['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        pred = prediction.tolist()\n",
    "        return(pred)\n",
    "    except Exception as e:\n",
    "        return(str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are 100 unique engine IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100]\n"
     ]
    }
   ],
   "source": [
    "print(score_df.id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The webservice test requires an `initialize` of the webservice, then send the entire scoring data set into the model. We expect to get 1 probability prediction for each engine in the scoring data set. Since the `score_df` has 100 machines, we expect 100 probabilities back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00375646841712296], [0.00033805289422161877], [0.0007565882406197488], [0.0003128668759018183], [6.648356793448329e-05], [0.0005828844732604921], [0.0002597148122731596], [0.00018564850324764848], [0.00030933888047002256], [7.479621126549318e-05], [0.00015490273653995246], [8.803210948826745e-05], [0.00010222086712019518], [0.0108488779515028], [0.5783206224441528], [0.00016159925144165754], [0.9886316657066345], [0.00016178519581444561], [4.838027962250635e-05], [0.9821566939353943], [9.348896855954081e-05], [0.00038087248685769737], [0.00023769873951096088], [0.0002285395748913288], [0.0005704331560991704], [0.9989494681358337], [0.004981394857168198], [0.0001682192669250071], [0.9991666078567505], [0.9897217154502869], [0.9333099126815796], [0.823441743850708], [0.0020724621135741472], [0.8350445628166199], [0.9192692041397095], [0.9986440539360046], [0.0010718094417825341], [0.0002111715730279684], [0.0007312165107578039], [0.05197881534695625], [0.00028019360615871847], [0.0003373870858922601], [0.9646533131599426], [0.00021823747374583036], [0.0005004195845685899], [0.19391988217830658], [0.5288172960281372], [4.95469466841314e-05], [9.529943054076284e-05], [0.9945275187492371], [0.00038826241507194936], [0.1641424149274826], [5.541891005123034e-05], [0.0002732418943196535], [0.9659560918807983], [0.0021574527490884066], [0.006673017051070929], [0.8595579266548157], [4.015572267235257e-05], [0.9910645484924316], [0.00012451615475583822], [0.9991338849067688], [0.00022303813602775335], [0.0004464489466045052], [0.0001356154098175466], [0.0019969483837485313], [9.186318493448198e-05], [0.0003400140267331153], [0.00026741219335235655], [0.9991658926010132], [0.5925479531288147], [2.764338205452077e-05], [0.0005784307140856981], [0.0005724497023038566], [0.9980504512786865], [0.9988222718238831], [0.00013012360432185233], [0.0034410227090120316], [0.00020106302690692246], [7.191860640887171e-05], [0.00035597695386968553], [0.00014851312153041363], [0.8678818941116333], [0.4361025094985962], [0.6202824711799622], [0.009777992032468319], [0.0030617788434028625], [0.00016564899124205112], [6.37207631370984e-05], [0.00016094985767267644], [0.0011740134796127677], [4.345101842773147e-05], [0.9340677261352539]]\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "\n",
    "prb=run(score_df)\n",
    "print(prb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we get 93, because 7 of the machines do not have the full 50 cycles available for scoring. If we send a machine with fewer than 50 records we get the following back: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error when checking : expected lstm_1_input to have 3 dimensions, but got array with shape (0, 1)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_df=score_df.loc[score_df['id'] == 1]\n",
    "\n",
    "print(tst_df.shape)\n",
    "\n",
    "# Because \n",
    "run(tst_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we send a complete data set, like machineID == 3, we get a probability back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 27)\n",
      "[[0.00375646841712296]]\n"
     ]
    }
   ],
   "source": [
    "tst_df=score_df.loc[score_df['id'] == 3]\n",
    "\n",
    "print(tst_df.shape)\n",
    "\n",
    "# Because \n",
    "ans=run(tst_df)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model assets\n",
    "\n",
    "Next we persist the assets we have created for use in operationalization. First we need to define the schema so the webservice knows what the payload data will look like as it comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"score_input\": SampleDefinition(DataTypes.PANDAS, score_df)}\n",
    "\n",
    "json_schema = generate_schema(run_func=run, inputs=inputs, filepath=SCHEMA_FILE)\n",
    "\n",
    "# save the schema file for deployment\n",
    "out = json.dumps(json_schema)\n",
    "with open(SHARE_ROOT + SCHEMA_FILE, 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conda dependencies are defined in this `webservices_conda.yaml` file. This will be used to tell the webservice server which python packages are required to run this web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /azureml-share//webservices_conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SHARE_ROOT}/webservices_conda.yaml\n",
    "\n",
    "# Conda environment specification. The dependencies defined in this file will\n",
    "# be automatically provisioned for managed runs. These include runs against\n",
    "# the localdocker, remotedocker, and cluster compute targets.\n",
    "\n",
    "# Note that this file is NOT used to automatically manage dependencies for the\n",
    "# local compute target. To provision these dependencies locally, run:\n",
    "# conda env update --file conda_dependencies.yml\n",
    "\n",
    "# Details about the Conda environment file format:\n",
    "# https://conda.io/docs/using/envs.html#create-environment-file-by-hand\n",
    "\n",
    "# For managing Spark packages and configuration, see spark_dependencies.yml.\n",
    "\n",
    "name: project_environment\n",
    "channels:\n",
    "- conda-forge\n",
    "- defaults\n",
    "dependencies:\n",
    "  - python=3.5.2\n",
    "  - pip:\n",
    "    - azure-common==1.1.8\n",
    "    - azure-storage==0.36.0\n",
    "    - numpy==1.14.0 \n",
    "    - sklearn\n",
    "    - keras\n",
    "    - tensorflow\n",
    "    - h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lstmscore.py` file is python code defining the web service operation. It includes both the `init()` and `run()` functions defined earlier imports the required libraries. These should be nearly identical to the previous defined versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /azureml-share//lstmscore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SHARE_ROOT}/lstmscore.py\n",
    "\n",
    "# import the libraries\n",
    "import keras\n",
    "import tensorflow\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "    global loaded_model\n",
    "    \n",
    "    # load json and create model\n",
    "    with open('modellstm.json', 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"modellstm.h5\")\n",
    "    loaded_model.compile('sgd','mse')\n",
    "\n",
    "def run(score_input):\n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "\n",
    "    input_features = score_input.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_input[score_input['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                 for id in score_input['id'].unique() if len(score_input[score_input['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        \n",
    "        pred = prediction.tolist()\n",
    "        return(pred)\n",
    "    except Exception as e:\n",
    "        return(str(e))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    run(\"{\\\"score_df\\\": [{\\\"s20\\\": 0.5581395348837184, \\\"s10\\\": 0.0, \\\"s2\\\": 0.5451807228915584, \\\"s21\\\": 0.6618337475835432, \\\"s9\\\": 0.12761374854168395, \\\"s19\\\": 0.0, \\\"s3\\\": 0.31066056245912677, \\\"cycle\\\": 1, \\\"s15\\\": 0.3089649865332831, \\\"s4\\\": 0.2694125590817009, \\\"s1\\\": 0.0, \\\"s11\\\": 0.2083333333333357, \\\"cycle_norm\\\": 0.0, \\\"s13\\\": 0.2205882352941444, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.2121212121210192, \\\"s14\\\": 0.1321601816492901, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.75, \\\"setting1\\\": 0.632183908045977, \\\"s12\\\": 0.6460554371002161, \\\"s17\\\": 0.3333333333333357, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.6521739130434696}, {\\\"s20\\\": 0.6821705426356601, \\\"s10\\\": 0.0, \\\"s2\\\": 0.15060240963856586, \\\"s21\\\": 0.6868268434134208, \\\"s9\\\": 0.14668401687158195, \\\"s19\\\": 0.0, \\\"s3\\\": 0.37955090473076325, \\\"cycle\\\": 2, \\\"s15\\\": 0.21315890727203168, \\\"s4\\\": 0.2223160027008788, \\\"s1\\\": 0.0, \\\"s11\\\": 0.38690476190476275, \\\"cycle_norm\\\": 0.002770083102493075, \\\"s13\\\": 0.26470588235270043, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.16666666666696983, \\\"s14\\\": 0.20476829394158358, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.25, \\\"setting1\\\": 0.3448275862068965, \\\"s12\\\": 0.7398720682302695, \\\"s17\\\": 0.4166666666666714, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.8051529790660226}, {\\\"s20\\\": 0.7286821705426334, \\\"s10\\\": 0.0, \\\"s2\\\": 0.3765060240963862, \\\"s21\\\": 0.7213476940071786, \\\"s9\\\": 0.15808130664991182, \\\"s19\\\": 0.0, \\\"s3\\\": 0.34663178548071016, \\\"cycle\\\": 3, \\\"s15\\\": 0.4586379376683354, \\\"s4\\\": 0.3222484807562438, \\\"s1\\\": 0.0, \\\"s11\\\": 0.38690476190476275, \\\"cycle_norm\\\": 0.0055401662049861505, \\\"s13\\\": 0.2205882352941444, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.22727272727297532, \\\"s14\\\": 0.15564041696769948, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.5833333333333334, \\\"setting1\\\": 0.5172413793103449, \\\"s12\\\": 0.6993603411513902, \\\"s17\\\": 0.4166666666666714, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.6859903381642596}]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also include a python file `test_service.py` which can test the web service you create. This program will test that there are enough cycles for the webservice to score, and send a single engine set to the web service. The results are printed with the engine ID to simulate how to use the webservice in a production setting.\n",
    "\n",
    "After you create the web service, you will need to edit this file and provide the correct `url` endpoint string before running this program.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /azureml-share//test_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SHARE_ROOT}/test_service.py\n",
    "\n",
    "import urllib\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# The URL will need to be editted after service create.\n",
    "url = 'http://127.0.0.1:32773/score'\n",
    "\n",
    "## Sequence length will need to match the training sequence length from\n",
    "## 2_model_building_and_evaluation.ipynb\n",
    "sequence_length = 50\n",
    "\n",
    "# We'll read in this data to test the service\n",
    "test_df = pd.read_pickle('PM_test_files.pkl')\n",
    "\n",
    "# Labels are what we're predicting.\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "# The scoreing data should not have labels... if we knew the label, \n",
    "# we wouldn't need to predict.\n",
    "score_df = test_df.drop(label_cols, axis = 1)\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# Now get the machine numbers, for each machine get the \n",
    "# prediction for the label timepoint\n",
    "machineID = score_df['id'].unique()\n",
    "\n",
    "for ind in machineID:\n",
    "    \n",
    "    try:\n",
    "        body = score_df[score_df.id==ind]\n",
    "        print('ID {}: size {}'.format(ind, body.shape))\n",
    "        if body.shape[0] < sequence_length : \n",
    "            print(\"Skipping machineID {} as we need {} records to score and only have {} records.\".format(ind, sequence_length, body.shape[0]))\n",
    "            continue\n",
    "        print('ID {}: {} \\t {}'.format(ind, body.shape, body.tail(sequence_length+ 10).shape))\n",
    "        body = \"{\\\"score_input\\\": \" + \\\n",
    "                    body.tail(sequence_length+10).to_json(orient=\"records\") +\\\n",
    "                    \"}\"\n",
    "        \n",
    "        req = urllib.request.Request(url, str.encode(body), headers) \n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            the_page = response.read()\n",
    "            print('ID {}: {}'.format(ind,the_page))\n",
    "        \n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code {}: \\n{}\".format(error, error.read))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(error.reason)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging\n",
    "\n",
    "To move the model artifacts around, we'll zip them into one file. We can then retrieve this file from the persistence shared folder on your DSVM.\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM_test_files.pkl  modellstm.h5    service_schema.json\twebservices_conda.yaml\r\n",
      "lstmscore.py\t   modellstm.json  test_service.py\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.storage.blob.models.ResourceProperties at 0x7fb24d428400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compress the operationalization assets for easy blob storage transfer\n",
    "# We can remove the persisted data files.\n",
    "# !rm {SHARE_ROOT}/PM*.pkl\n",
    "!rm {SHARE_ROOT}/PM_train_files.pkl\n",
    "!ls {SHARE_ROOT}\n",
    "\n",
    "MODEL_O16N = shutil.make_archive('LSTM_o16n', 'zip', SHARE_ROOT)\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(MODEL_CONTAINER,\n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# Transfer the compressed operationalization assets into the blob container.\n",
    "az_blob_service.create_blob_from_path(MODEL_CONTAINER, \"LSTM_o16n.zip\", str(MODEL_O16N)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Once the assets are stored, we can download them into a deployment compute context for operationalization on an Azure web service. For this scenario, we will deploy this on our local docker container context.\n",
    "\n",
    "We demonstrate how to setup this web service this through a CLI window opened in the AML Workbench application. \n",
    "\n",
    "## Download the model\n",
    "\n",
    "To download the model we've saved, follow these instructions on a local computer.\n",
    "\n",
    " - Open the Azure Portal\n",
    " - In the left hand pane, click on All resources\n",
    " - Search for the storage account using the name you provided earlier in this notebook.\n",
    " - Choose the storage account from search result list, this will open the storage account panel.\n",
    " - On the storage account panel, choose Blobs\n",
    " - On the Blobs panel choose the container `pmlstmmodel`\n",
    " - Select the file `LSTM_o16n.zip` and on the properties pane for that blob, choose download.\n",
    "\n",
    "Once downloaded, unzip the file into the directory of your choosing. The zip file contains the deployment assets:\n",
    "\n",
    "- the `lstmscore.py` file which contains functionst to do the model scoring\n",
    "- the `modellstm.json` model definition file\n",
    "- the `modellstm.h5` model weights file\n",
    "- the `service_schema.json` which defines the input data schema\n",
    "- the `webservices_conda.yaml` defining which python packages are required to run this service.\n",
    "- the `test_service.py` script for testing your webservice once deployed.\n",
    "- the `PM_test_df.pkl` test data set used by the `test_service.py` script\n",
    "\n",
    "\n",
    "## Create a model management endpoint \n",
    "\n",
    "Create a modelmanagement under your account. We will call this `pdmmodelmanagement`. You only need to do this once for any AMLWB models you would like to deploy. You will need to supply an **ACCOUNT_REGION** and **RESOURCE_GROUP** from your Azure account. The remaining defaults are acceptable.\n",
    "\n",
    "`az ml account modelmanagement create --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "You can find the correct text for the `<ACCOUNT_REGION>` using the command:\n",
    "\n",
    "`az account list-locations`\n",
    "\n",
    "If you get a `ResourceGroupNotFound` error, you may need to set the correct subscription. This is typically only an issue if your Azure login connects to multiple subscritpions. \n",
    "\n",
    "`az account set -s '<subscription name>'`\n",
    "\n",
    "You can find the `subscription name` or `subscription id` through the (https://portal.azure.com) under the resource group you'd like to use.\n",
    "\n",
    "## Check environment settings\n",
    "\n",
    "Show what environment is currently active:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "If nothing is set, we setup the environment with an existing model management context (created above) first: \n",
    "\n",
    "` az ml env setup --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "using the same `<ACCOUNT_REGION>` and `<RESOURCE_GROUP>` in the previous section. Then set the current environment:\n",
    "\n",
    "`az ml env set --resource-group <RESOURCE_GROUP> --cluster-name pdmmodelmanagement`\n",
    "\n",
    "Check that the environment is now set:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "## Deploy a web service \n",
    "\n",
    "These commands assume the current directory contains the webservice assets we created in throughout the notebooks in this scenario (at least `lstmscore.py`, `modellstm.json`, `modellstm.h5`, `service_schema.json` and `webservices_conda.yaml`). If not, in the AML CLI window, change to the directory where the zip file was unpacked. \n",
    "\n",
    "The command to create a web service (`<SERVICE_ID>`) with these operationalization assets in the current directory is:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f <filename> -r <TARGET_RUNTIME> -m <MODEL_FILE> -s <SCHEMA_FILE> -n <SERVICE_ID> --cpu 0.1\n",
    "`\n",
    "\n",
    "The default cluster has only 2 nodes with 2 cores each. Some cores are taken for system components. AMLWorkbench asks for 1 core per service. To deploy multiple services into this cluster, we specify the cpu requirement in the service create command as (--cpu 0.1) to request 10% of a core. \n",
    "\n",
    "For this example, we will call our webservice with the `SERVICE_ID` = `lstmwebservice`. The `SERVICE_ID` must be all lowercase, with no spaces. This command should work with your account and the deployment artifacts created in this notebook.\n",
    "\n",
    "`\n",
    "az ml service create realtime -f lstmscore.py -r python -m modellstm.json -m modellstm.h5 -s service_schema.json -c webservices_conda.yaml --cpu 0.1 -n lstmwebservice\n",
    "`\n",
    "\n",
    "This command will take some time to execute. \n",
    "\n",
    "## Test your deployment.\n",
    "\n",
    "Once complete, the `az ml service create` command returns sample usage commands to test the service for both PowerShell and the cmd prompt. You can copy and paste this command into the CLI to test the web service. However, since it is only an example with 1 cycle, we know the model can not return a reasonable score. Instead you will see the following error as detailed above:\n",
    "```\n",
    "'Error when checking : expected lstm_1_input to have 3 dimensions, but got array with shape (0, 1)'\n",
    "\n",
    "```\n",
    "We have provided the `test_service.py` python script to send larger payloads to the service. First obtain the webservice endpoint with the following command. `az ml service usage realtime -i lstmwebservice`\n",
    "\n",
    "```\n",
    "> az ml service usage realtime -i lstmwebservice\n",
    "Scoring URL:\n",
    "    http://127.0.0.1:32770/score\n",
    "\n",
    "Headers:\n",
    "    Content-Type: application/json\n",
    "\n",
    "Swagger URL:\n",
    "    http://127.0.0.1:32770/swagger.json\n",
    "\n",
    "Sample CLI command:\n",
    "...\n",
    "```\n",
    "\n",
    "Copy the Scoring URL into the `url` variable in the `test_service.py` script and save the file.\n",
    "\n",
    "From the CLI, you can then run the script with the command and response similar to the following:\n",
    "\n",
    "`python test_service.py`\n",
    "\n",
    "```\n",
    "ID 1: size (31, 27)\n",
    "Skipping machineID 1 as we need 50 records to score and only have 31 records.\n",
    "ID 2: size (49, 27)\n",
    "Skipping machineID 2 as we need 50 records to score and only have 49 records.\n",
    "ID 3: size (126, 27)\n",
    "ID 3: (126, 27)          (60, 27)\n",
    "ID 3: b'\"[[0.003690145444124937]]\"'\n",
    "ID 4: size (106, 27)\n",
    "ID 4: (106, 27)          (60, 27)\n",
    "ID 4: b'\"[[0.0014941827394068241]]\"'\n",
    "ID 5: size (98, 27)\n",
    "ID 5: (98, 27)   (60, 27)\n",
    "ID 5: b'\"[[0.002231441903859377]]\"'\n",
    "ID 6: size (105, 27)\n",
    "ID 6: (105, 27)          (60, 27)\n",
    "ID 6: b'\"[[0.001609073020517826]]\"'\n",
    "ID 7: size (160, 27)\n",
    "ID 7: (160, 27)          (60, 27)\n",
    "ID 7: b'\"[[0.001093236613087356]]\"'\n",
    "ID 8: size (166, 27)\n",
    "ID 8: (166, 27)          (60, 27)\n",
    "ID 8: b'\"[[0.0017530957702547312]]\"'\n",
    "ID 9: size (55, 27)\n",
    "ID 9: (55, 27)   (55, 27)\n",
    "ID 9: b'\"[[0.0015462590381503105]]\"'\n",
    "ID 10: size (192, 27)\n",
    "ID 10: (192, 27)         (60, 27)\n",
    "ID 10: b'\"[[0.0014192759990692139]]\"'\n",
    "ID 11: size (83, 27)\n",
    "ID 11: (83, 27)          (60, 27)\n",
    "ID 11: b'\"[[0.0017256727442145348]]\"'\n",
    "ID 12: size (217, 27)\n",
    "ID 12: (217, 27)         (60, 27)\n",
    "ID 12: b'\"[[0.0011249856324866414]]\"'\n",
    "ID 13: size (195, 27)\n",
    "ID 13: (195, 27)         (60, 27)\n",
    "ID 13: b'\"[[0.0013882251223549247]]\"'\n",
    "ID 14: size (46, 27)\n",
    "Skipping machineID 14 as we need 50 records to score and only have 46 records.\n",
    "ID 15: size (76, 27)\n",
    "ID 15: (76, 27)          (60, 27)\n",
    "ID 15: b'\"[[0.0010017495369538665]]\"'\n",
    "ID 16: size (113, 27)\n",
    "ID 16: (113, 27)         (60, 27)\n",
    "ID 16: b'\"[[0.0011311823036521673]]\"'\n",
    "ID 17: size (165, 27)\n",
    "ID 17: (165, 27)         (60, 27)\n",
    "ID 17: b'\"[[0.004585478454828262]]\"'\n",
    "ID 18: size (133, 27)\n",
    "ID 18: (133, 27)         (60, 27)\n",
    "ID 18: b'\"[[0.08827716112136841]]\"'\n",
    "ID 19: size (135, 27)\n",
    "ID 19: (135, 27)         (60, 27)\n",
    "ID 19: b'\"[[0.0012705528642982244]]\"'\n",
    "ID 20: size (184, 27)\n",
    "ID 20: (184, 27)         (60, 27)\n",
    "ID 20: b'\"[[0.531469464302063]]\"'\n",
    "ID 21: size (148, 27)\n",
    "ID 21: (148, 27)         (60, 27)\n",
    "ID 21: b'\"[[0.0013451582053676248]]\"'\n",
    "ID 22: size (39, 27)\n",
    "Skipping machineID 22 as we need 50 records to score and only have 39 records.\n",
    "ID 23: size (130, 27)\n",
    "ID 23: (130, 27)         (60, 27)\n",
    "ID 23: b'\"[[0.0008326334645971656]]\"'\n",
    "ID 24: size (186, 27)\n",
    "ID 24: (186, 27)         (60, 27)\n",
    "ID 24: b'\"[[0.8084099888801575]]\"'\n",
    "ID 25: size (48, 27)\n",
    "Skipping machineID 25 as we need 50 records to score and only have 48 records.\n",
    "ID 26: size (76, 27)\n",
    "ID 26: (76, 27)          (60, 27)\n",
    "ID 26: b'\"[[0.0009542660554870963]]\"'\n",
    "ID 27: size (140, 27)\n",
    "ID 27: (140, 27)         (60, 27)\n",
    "ID 27: b'\"[[0.0021106707863509655]]\"'\n",
    "ID 28: size (158, 27)\n",
    "ID 28: (158, 27)         (60, 27)\n",
    "ID 28: b'\"[[0.0013031685957685113]]\"'\n",
    "ID 29: size (171, 27)\n",
    "ID 29: (171, 27)         (60, 27)\n",
    "ID 29: b'\"[[0.0016013996209949255]]\"'\n",
    "ID 30: size (143, 27)\n",
    "ID 30: (143, 27)         (60, 27)\n",
    "ID 30: b'\"[[0.001739262486808002]]\"'\n",
    "ID 31: size (196, 27)\n",
    "ID 31: (196, 27)         (60, 27)\n",
    "ID 31: b'\"[[0.9811599850654602]]\"'\n",
    "ID 32: size (145, 27)\n",
    "ID 32: (145, 27)         (60, 27)\n",
    "ID 32: b'\"[[0.004659396130591631]]\"'\n",
    "ID 33: size (50, 27)\n",
    "ID 33: (50, 27)          (50, 27)\n",
    "ID 33: b'\"[[0.0014037484070286155]]\"'\n",
    "ID 34: size (203, 27)\n",
    "ID 34: (203, 27)         (60, 27)\n",
    "ID 34: b'\"[[0.9670061469078064]]\"'\n",
    "ID 35: size (198, 27)\n",
    "ID 35: (198, 27)         (60, 27)\n",
    "ID 35: b'\"[[0.9815614819526672]]\"'\n",
    "ID 36: size (126, 27)\n",
    "ID 36: (126, 27)         (60, 27)\n",
    "ID 36: b'\"[[0.6011714339256287]]\"'\n",
    "ID 37: size (121, 27)\n",
    "ID 37: (121, 27)         (60, 27)\n",
    "ID 37: b'\"[[0.09062018990516663]]\"'\n",
    "ID 38: size (125, 27)\n",
    "ID 38: (125, 27)         (60, 27)\n",
    "ID 38: b'\"[[0.004488768056035042]]\"'\n",
    "ID 39: size (37, 27)\n",
    "Skipping machineID 39 as we need 50 records to score and only have 37 records.\n",
    "ID 40: size (133, 27)\n",
    "ID 40: (133, 27)         (60, 27)\n",
    "ID 40: b'\"[[0.2266664057970047]]\"'\n",
    "ID 41: size (123, 27)\n",
    "ID 41: (123, 27)         (60, 27)\n",
    "ID 41: b'\"[[0.34560075402259827]]\"'\n",
    "ID 42: size (156, 27)\n",
    "ID 42: (156, 27)         (60, 27)\n",
    "ID 42: b'\"[[0.9412360191345215]]\"'\n",
    "ID 43: size (172, 27)\n",
    "ID 43: (172, 27)         (60, 27)\n",
    "ID 43: b'\"[[0.0023652282543480396]]\"'\n",
    "ID 44: size (54, 27)\n",
    "ID 44: (54, 27)          (54, 27)\n",
    "ID 44: b'\"[[0.0016468992689624429]]\"'\n",
    "ID 45: size (152, 27)\n",
    "ID 45: (152, 27)         (60, 27)\n",
    "ID 45: b'\"[[0.0028006499633193016]]\"'\n",
    "ID 46: size (146, 27)\n",
    "ID 46: (146, 27)         (60, 27)\n",
    "ID 46: b'\"[[0.01627732440829277]]\"'\n",
    "ID 47: size (73, 27)\n",
    "ID 47: (73, 27)          (60, 27)\n",
    "ID 47: b'\"[[0.001768825575709343]]\"'\n",
    "ID 48: size (78, 27)\n",
    "ID 48: (78, 27)          (60, 27)\n",
    "ID 48: b'\"[[0.0014678625157102942]]\"'\n",
    "ID 49: size (303, 27)\n",
    "ID 49: (303, 27)         (60, 27)\n",
    "ID 49: b'\"[[0.20870330929756165]]\"'\n",
    "ID 50: size (74, 27)\n",
    "ID 50: (74, 27)          (60, 27)\n",
    "ID 50: b'\"[[0.0012800053227692842]]\"'\n",
    "ID 51: size (144, 27)\n",
    "ID 51: (144, 27)         (60, 27)\n",
    "ID 51: b'\"[[0.0018505703192204237]]\"'\n",
    "ID 52: size (189, 27)\n",
    "ID 52: (189, 27)         (60, 27)\n",
    "ID 52: b'\"[[0.05433798208832741]]\"'\n",
    "ID 53: size (164, 27)\n",
    "ID 53: (164, 27)         (60, 27)\n",
    "ID 53: b'\"[[0.043289054185152054]]\"'\n",
    "ID 54: size (121, 27)\n",
    "ID 54: (121, 27)         (60, 27)\n",
    "ID 54: b'\"[[0.0008320639608427882]]\"'\n",
    "ID 55: size (113, 27)\n",
    "ID 55: (113, 27)         (60, 27)\n",
    "ID 55: b'\"[[0.0010971747105941176]]\"'\n",
    "ID 56: size (136, 27)\n",
    "ID 56: (136, 27)         (60, 27)\n",
    "ID 56: b'\"[[0.935263991355896]]\"'\n",
    "ID 57: size (160, 27)\n",
    "ID 57: (160, 27)         (60, 27)\n",
    "ID 57: b'\"[[0.001842512865550816]]\"'\n",
    "ID 58: size (176, 27)\n",
    "ID 58: (176, 27)         (60, 27)\n",
    "ID 58: b'\"[[0.008606999181210995]]\"'\n",
    "ID 59: size (94, 27)\n",
    "ID 59: (94, 27)          (60, 27)\n",
    "ID 59: b'\"[[0.000816409767139703]]\"'\n",
    "ID 60: size (147, 27)\n",
    "ID 60: (147, 27)         (60, 27)\n",
    "ID 60: b'\"[[0.0018146623624488711]]\"'\n",
    "ID 61: size (159, 27)\n",
    "ID 61: (159, 27)         (60, 27)\n",
    "ID 61: b'\"[[0.16924026608467102]]\"'\n",
    "ID 62: size (232, 27)\n",
    "ID 62: (232, 27)         (60, 27)\n",
    "ID 62: b'\"[[0.006756726652383804]]\"'\n",
    "ID 63: size (155, 27)\n",
    "ID 63: (155, 27)         (60, 27)\n",
    "ID 63: b'\"[[0.004599822219461203]]\"'\n",
    "ID 64: size (168, 27)\n",
    "ID 64: (168, 27)         (60, 27)\n",
    "ID 64: b'\"[[0.1045515388250351]]\"'\n",
    "ID 65: size (71, 27)\n",
    "ID 65: (71, 27)          (60, 27)\n",
    "ID 65: b'\"[[0.0006783730932511389]]\"'\n",
    "ID 66: size (147, 27)\n",
    "ID 66: (147, 27)         (60, 27)\n",
    "ID 66: b'\"[[0.5332847237586975]]\"'\n",
    "ID 67: size (71, 27)\n",
    "ID 67: (71, 27)          (60, 27)\n",
    "ID 67: b'\"[[0.0010836098808795214]]\"'\n",
    "ID 68: size (187, 27)\n",
    "ID 68: (187, 27)         (60, 27)\n",
    "ID 68: b'\"[[0.9780803918838501]]\"'\n",
    "ID 69: size (54, 27)\n",
    "ID 69: (54, 27)          (54, 27)\n",
    "ID 69: b'\"[[0.0013221375411376357]]\"'\n",
    "ID 70: size (152, 27)\n",
    "ID 70: (152, 27)         (60, 27)\n",
    "ID 70: b'\"[[0.0018269383581355214]]\"'\n",
    "ID 71: size (68, 27)\n",
    "ID 71: (68, 27)          (60, 27)\n",
    "ID 71: b'\"[[0.0011093434877693653]]\"'\n",
    "ID 72: size (131, 27)\n",
    "ID 72: (131, 27)         (60, 27)\n",
    "ID 72: b'\"[[0.0025075972080230713]]\"'\n",
    "ID 73: size (112, 27)\n",
    "ID 73: (112, 27)         (60, 27)\n",
    "ID 73: b'\"[[0.0010514295427128673]]\"'\n",
    "ID 74: size (137, 27)\n",
    "ID 74: (137, 27)         (60, 27)\n",
    "ID 74: b'\"[[0.001497327582910657]]\"'\n",
    "ID 75: size (88, 27)\n",
    "ID 75: (88, 27)          (60, 27)\n",
    "ID 75: b'\"[[0.0013855294091627002]]\"'\n",
    "ID 76: size (205, 27)\n",
    "ID 76: (205, 27)         (60, 27)\n",
    "ID 76: b'\"[[0.9812294244766235]]\"'\n",
    "ID 77: size (162, 27)\n",
    "ID 77: (162, 27)         (60, 27)\n",
    "ID 77: b'\"[[0.03927205502986908]]\"'\n",
    "ID 78: size (72, 27)\n",
    "ID 78: (72, 27)          (60, 27)\n",
    "ID 78: b'\"[[0.0005832962924614549]]\"'\n",
    "ID 79: size (101, 27)\n",
    "ID 79: (101, 27)         (60, 27)\n",
    "ID 79: b'\"[[0.0019143277313560247]]\"'\n",
    "ID 80: size (133, 27)\n",
    "ID 80: (133, 27)         (60, 27)\n",
    "ID 80: b'\"[[0.0017474571941420436]]\"'\n",
    "ID 81: size (213, 27)\n",
    "ID 81: (213, 27)         (60, 27)\n",
    "ID 81: b'\"[[0.9732806086540222]]\"'\n",
    "ID 82: size (162, 27)\n",
    "ID 82: (162, 27)         (60, 27)\n",
    "ID 82: b'\"[[0.9421394467353821]]\"'\n",
    "ID 83: size (73, 27)\n",
    "ID 83: (73, 27)          (60, 27)\n",
    "ID 83: b'\"[[0.0011075431248173118]]\"'\n",
    "ID 84: size (172, 27)\n",
    "ID 84: (172, 27)         (60, 27)\n",
    "ID 84: b'\"[[0.0032924318220466375]]\"'\n",
    "ID 85: size (34, 27)\n",
    "Skipping machineID 85 as we need 50 records to score and only have 34 records.\n",
    "ID 86: size (110, 27)\n",
    "ID 86: (110, 27)         (60, 27)\n",
    "ID 86: b'\"[[0.001197033328935504]]\"'\n",
    "ID 87: size (56, 27)\n",
    "ID 87: (56, 27)          (56, 27)\n",
    "ID 87: b'\"[[0.0008590950164943933]]\"'\n",
    "ID 88: size (68, 27)\n",
    "ID 88: (68, 27)          (60, 27)\n",
    "ID 88: b'\"[[0.001377861830405891]]\"'\n",
    "ID 89: size (177, 27)\n",
    "ID 89: (177, 27)         (60, 27)\n",
    "ID 89: b'\"[[0.0012676144251599908]]\"'\n",
    "ID 90: size (146, 27)\n",
    "ID 90: (146, 27)         (60, 27)\n",
    "ID 90: b'\"[[0.13571304082870483]]\"'\n",
    "ID 91: size (234, 27)\n",
    "ID 91: (234, 27)         (60, 27)\n",
    "ID 91: b'\"[[0.2942765951156616]]\"'\n",
    "ID 92: size (150, 27)\n",
    "ID 92: (150, 27)         (60, 27)\n",
    "ID 92: b'\"[[0.3088526129722595]]\"'\n",
    "ID 93: size (244, 27)\n",
    "ID 93: (244, 27)         (60, 27)\n",
    "ID 93: b'\"[[0.0115140900015831]]\"'\n",
    "ID 94: size (133, 27)\n",
    "ID 94: (133, 27)         (60, 27)\n",
    "ID 94: b'\"[[0.004197425674647093]]\"'\n",
    "ID 95: size (89, 27)\n",
    "ID 95: (89, 27)          (60, 27)\n",
    "ID 95: b'\"[[0.0012697000056505203]]\"'\n",
    "ID 96: size (97, 27)\n",
    "ID 96: (97, 27)          (60, 27)\n",
    "ID 96: b'\"[[0.0008713220595382154]]\"'\n",
    "ID 97: size (134, 27)\n",
    "ID 97: (134, 27)         (60, 27)\n",
    "ID 97: b'\"[[0.001297763898037374]]\"'\n",
    "ID 98: size (121, 27)\n",
    "ID 98: (121, 27)         (60, 27)\n",
    "ID 98: b'\"[[0.0026060494128614664]]\"'\n",
    "ID 99: size (97, 27)\n",
    "ID 99: (97, 27)          (60, 27)\n",
    "ID 99: b'\"[[0.0007261313730850816]]\"'\n",
    "ID 100: size (198, 27)\n",
    "ID 100: (198, 27)        (60, 27)\n",
    "ID 100: b'\"[[0.2326587438583374]]\"'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "Working through all of these notebooks, we have completed:\n",
    "\n",
    " * Data aquisition ingestion and preparation in `Code/1_data_ingestion_and_preparation.ipynb` notebook.\n",
    " * Building an LSTM time series model that used a 50 cycle history from an aircraft engine to predict failure within the next 30 cycles in the `Code/2_model_building_and_evaluation.ipynb` notebook.\n",
    " * Operationalization asset generation and model deployment in the `Code/3_operationalization.ipynb` notebook.\n",
    " \n",
    "This scenario is intended to help guide you through the predictive maintenance model development process with your own data. \n",
    "\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "LSTM miniDSVM",
   "language": "python",
   "name": "lstm_minidsvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
